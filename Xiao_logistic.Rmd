---
title: "Group 1 Final"
author: "Evelyn Campo, Xiao Qi, Nusrat Prithee, Roman Kosarzycki"
date: "today"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
library(ezids)
library(ggplot2)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
```
# Model building
## SMART Question
What variables affect instances of heart disease?

## Pre-processing amd balancing the data
Our goal is to find out the people who are likely to have heart disease in the future, so we can take some actions like a more detailed physical examination before the conditions become worse.
```{r}
heartdata <- data.frame(read.csv("heart_2020_cleaned.csv"))

n=10
heartdata$Smoking <- as.factor(heartdata$Smoking)
heartdata$AlcoholDrinking <- as.factor(heartdata$AlcoholDrinking)
heartdata$Stroke <- as.factor(heartdata$Stroke)
heartdata$DiffWalking <- as.factor(heartdata$DiffWalking)
heartdata$Sex <- as.factor(heartdata$Sex)
heartdata$Race <- as.factor(heartdata$Race)

heartdata[heartdata$Diabetic == 'Yes (during pregnancy)', "Diabetic"] <- 'Yes'
heartdata$Diabetic <- as.factor(heartdata$Diabetic)

heartdata$PhysicalActivity <- as.factor(heartdata$PhysicalActivity)

heartdata[heartdata$GenHealth == 'Poor', "GenHealth"] <- 0
heartdata[heartdata$GenHealth == 'Fair', "GenHealth"] <- 1
heartdata[heartdata$GenHealth == 'Good', "GenHealth"] <- 2
heartdata[heartdata$GenHealth == 'Very good', "GenHealth"] <- 3
heartdata[heartdata$GenHealth == 'Excellent', "GenHealth"] <- 4
#heartdata$GenHealth <- as.factor(heartdata$GenHealth)
heartdata$GenHealth <- as.numeric(heartdata$GenHealth)

heartdata$Asthma <- as.factor(heartdata$Asthma)
heartdata$KidneyDisease <- as.factor(heartdata$KidneyDisease)
heartdata$SkinCancer <- as.factor(heartdata$SkinCancer)

heartdata$AgeCategory <- gsub(" or older","-4",as.character(heartdata$AgeCategory))
heartdata$LoAge <- as.numeric(substr(heartdata$AgeCategory,1,2))
heartdata$rand <- sample(0:4,size=nrow(heartdata),replace=T)
heartdata$Age <- with(heartdata,LoAge+rand)
heartdata <- subset(heartdata,select=-c(AgeCategory,LoAge,rand))

heartdata$y <- as.factor(heartdata$HeartDisease)
heartdata = subset(heartdata, select = -c(HeartDisease))

str(heartdata)
```
At first, because we need to do the feature selection later, I put the HeartDisease column in the end of the dataset and rename it into y.

To get a look of the proportion of heart disease data before we continue our research.
```{r results='markup'}
table( heartdata$y )
```
The dataset is very unbalanced, Considering that the dataset is large, so I use undersampling methods to balance the dataset. Reference: https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/

```{r}
loadPkg("ROSE")
data_balanced_under <- ovun.sample(y ~ ., data = heartdata, method = "under", N = 27373*2, seed = 1)$data
rm(heartdata)
unloadPkg("ROSE") 
```

We can find that the data is really balanced.
```{r results='markup'}
table( data_balanced_under$y )
```

And also the structure of the dataset.
```{r results='markup'}
str(data_balanced_under)
```

## logistic regression model

To train and evaluate the model, we will split the dataset into two parts. 80 percent of the dataset will be used to train the model and the rest will be used to test the accuracy of the model.
```{r}
loadPkg("caret")
set.seed(4321)
test <- createDataPartition( data_balanced_under$y, p = .2, list = FALSE )
data_train <- data_balanced_under[ -test, ]
data_test  <- data_balanced_under[ test, ]
unloadPkg("caret") 
```

```{r results='markup'}
model_glm <- glm( y ~ ., data = data_train, family = binomial(logit) )
summary_glm <- summary(model_glm)
summary_glm
```

We can find from the model that the p-value of Race and PhysicalActivity are larger than 0.05, which means they are insignificant. So just drop these two variables and make a logistic regression model again.

```{r results='markup'}
model2_glm <- glm( y ~ . - Race - PhysicalActivity, data = data_train, family = binomial(logit) )
summary2_glm <- summary(model2_glm)
summary2_glm
```

We’ll quickly check two things for this model. First the p-values. Values below .05 indicates significance, which means the coefficient or so called parameters that are estimated by our model are reliable. And second, the pseudo R square. This value ranging from 0 to 1 indicates how much variance is explained by our model.

All the p-values of the models indicates significance, meaning that our model is a legitimate one. But R square of 0.29 tells that 29 percent of the variance is explained.
```{r}
list( summary2_glm$coefficient, 
      round( 1 - ( summary2_glm$deviance / summary2_glm$null.deviance ), 2 ) )
```

Have a look of the vif value.
```{r results='markup'}
vif_md2 = faraway::vif(model2_glm)
vif_md2
```
We can find that some vif values are larger than 10, which means these variables are highly correlated, not acceptable. So I tried to drop one variable a time. At mean time, look at the p-value to ensure that the variables are significant. At the end, we got the model like below:

```{r results='markup'}
model3_glm <- glm( y ~ . - Race - PhysicalActivity - Age - Asthma - PhysicalHealth, data = data_train, family = binomial(logit) )
summary3_glm <- summary(model3_glm)
summary3_glm
```

```{r results='markup'}
vif_md3 = faraway::vif(model3_glm)
vif_md3
```
Now, the vif values are all below 10.

## Feature selection
In this part, I want to use feature selection to find out the suitable variables from our current model. Because the data_train dataset is large and takes a lot of time to run. I changed to data_test to do the feature selection.

```{r}
data_feature_selection = subset(data_test, select = -c(Race, PhysicalActivity, PhysicalHealth, Age, Asthma))
str(data_feature_selection)
```

```{r results='markup'}
loadPkg("bestglm")
res.bestglm <- bestglm(Xy = data_feature_selection, family = binomial,
            IC = "AIC",                 # Information criteria for
            method = "backward")
summary(res.bestglm)
res.bestglm$BestModels
summary(res.bestglm$BestModels)
unloadPkg("bestglm") 
unloadPkg("leaps") 
```

We can find from the feature selection that the best model has all these 13 variables, and its CIA (Akaike Information Criterion) is 12062, which is the lowest among these models.

## Model Evaluation

### ROC and AUC

Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC) measures the true positive rate (or sensitivity) against the false positive rate (or specificity). The area-under-curve is always between 0.5 and 1. Values higher than 0.8 is considered good model fit.  
```{r roc_auc}
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(model3_glm, type = "response" )
data_train$prob=prob
h <- roc(y~prob, data=data_train)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
```
The AUC of the model is 0.795, which is a little bit lower than 0.8. Because our model looks suitalbe and we have all the needed features, I suppose that the data causes the AUC value is not high enough. 


#### Confusion matrix 

We can have a look of the Confusion matrix.

```{r confusionMatrix, results='markup'}
loadPkg("regclass")
# confusion_matrix(admitLogit)
xkabledply( confusion_matrix(model3_glm), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")
```
We can find from the confusion Matrix that Precision is 14941/(5252+14941) = 0.74, which means the valid of the result is 0.74. And the recall is 14941/(6957+14941) = 0.682, which means how complete the results are. In our model, actually, I think the recall is more important, because FN means heart disease patients who are missed by our model, which can cause a harmful result.


```{r results='markup'}
loadPkg("InformationValue")
predicted = predict(model3_glm, data_test, type = "response")

confusionMatrix(data_test$y, predicted)
unloadPkg("InformationValue")
```
Then we can use the test dataset to checkout whether the model is good to use. So I used the data_test to make a prediction and calculate the confusion Matrix by the test dataset. The Precision is 3722/(1255+3722) = 0.748 and the recall is 3722/(1753+3722) = 0.68. The Precision value and recall value of the test dataset is similiar to the train dataset, which means our model is reliable to predict the heart disease.


## Interpretation and Reporting

We’ll return to our logistic regression model for a minute, and look at the estiamted parameters (coefficients). Since the model’s parameter the recorded in logit format, we’ll transform it into odds ratio so that it’ll be easier to interpret.


```{r results='markup'}
loadPkg("broom")

coefficient <- tidy(model3_glm)[ , c( "term", "estimate", "statistic" ) ]

# transfrom the coefficient to be in probability format 
coefficient$estimate <- exp( coefficient$estimate )
coefficient[sort(abs(coefficient$estimate),decreasing=T,index.return=T)[[2]],]

unloadPkg("broom")
```

We can find from the table that other diseases (stroke, kidney disease, diabetic, SkinCancer), general health conditions, sex, DiffWalking (serious difficulty walking or climbing stairs), and smoking habit all largely influence the possibility of heart disease. It's a little weird that drinking alcohol will reduce the possibility of heart disease. As we always think, drinking is not a good habit, maybe the data also includes people who drink some little wine.

